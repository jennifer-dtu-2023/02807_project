{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f7b3d6",
   "metadata": {},
   "source": [
    "---\n",
    "# **Final Project** | 02807 Computational Tools for Data Science\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff7063",
   "metadata": {},
   "source": [
    "# **Table of contents**\n",
    "1. [How to read this notebook](#HowToRead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbee4f",
   "metadata": {},
   "source": [
    "# **How to read this notebook** <a name=\"HowToRead\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9c57d",
   "metadata": {},
   "source": [
    "- The whole code is executable. Nevertheless, certain sections may require a more extended period to execute than others. Consequently, we advise against running the entire script in one go, unless waiting is not a concern for you. Instead, we recommend executing one cell at a time. In instances where a cell requires a longer computation time (**More than 1 minute**), a warning will be displayed as follows: **<span style = \"color:red\"> Warning: This takes approximately [X] minutes to run**. Please be guided accordingly.\n",
    "\n",
    "**About the project and structure of the notebook:**\n",
    "- This project encompasses the development of a recommendation system that includes an interactive program. The primary objective is to provide movie recommendations to a user based on a previously watched movie. The structure of this notebook follows the process through which a final set of movie recommendations is generated for a user, predicated on a movie they have previously watched. The overall design of this system is as follows:\n",
    "1. Read the movie data.\n",
    "2. The data from *step 1* is then filtered based on genre.\n",
    "3. The data from *step 2* is then filtered based on *network communities*.\n",
    "4. The data from *step 3* is then filtered based on *topic modeling*.\n",
    "5. The data from *step 4* is then filtered based on *similar users*.\n",
    "6. Finally 5 movies will be recommended for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5483c1",
   "metadata": {},
   "source": [
    "# **Importing libraries**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e0cfc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/botocore/httpsession.py:41: DeprecationWarning: 'urllib3.contrib.pyopenssl' module is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680\n",
      "  from urllib3.contrib.pyopenssl import orig_util_SSLContext as SSLContext\n",
      "/opt/anaconda3/lib/python3.9/site-packages/gensim/matutils.py:22: DeprecationWarning: Please use `triu` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n",
      "  from scipy.linalg.special_matrices import triu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "import math\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from wordcloud import WordCloud\n",
    "import colorsys\n",
    "from scipy.spatial import distance\n",
    "import contextlib\n",
    "\n",
    "\n",
    "# Networkx and community\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import girvan_newman\n",
    "import community\n",
    "import community.community_louvain as cl\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "with contextlib.redirect_stdout(None):\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    \n",
    "# Interactive system\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox, Scrollbar, Listbox, StringVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbc2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable variables\n",
    "seed = 33\n",
    "d_color = \"#4152ec\"\n",
    "network_color_map = \"turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ab397",
   "metadata": {},
   "source": [
    "# **Data loading**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45af676f-173b-4f98-bf37-b7b0bf6e79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./data/Netflix_Dataset_Movie.csv' exists ✅\n",
      "---\n",
      "'./data/Netflix_Dataset_Rating.csv' exists ✅\n",
      "---\n",
      "'./data/tmdb_5000_credits.csv' exists ✅\n",
      "---\n",
      "'./data/tmdb_5000_movies.csv' exists ✅\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# used csv files, linking it with their source zip files\n",
    "csv_to_zip_source = {\n",
    "    \"Netflix_Dataset_Movie.csv\": \"zip_sources/Netflix_Dataset_Movie.csv.zip\",\n",
    "    \"Netflix_Dataset_Rating.csv\": \"zip_sources/Netflix_Dataset_Rating.csv.zip\",\n",
    "    \"tmdb_5000_credits.csv\": \"zip_sources/tmdb.zip\",\n",
    "    \"tmdb_5000_movies.csv\": \"zip_sources/tmdb.zip\"\n",
    "}\n",
    "\n",
    "csv_file_names = list(csv_to_zip_source.keys())\n",
    "\n",
    "# common data directory path\n",
    "data_dir = \"./data\"\n",
    "\n",
    "# if directory does not exist, create it\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "# check if all .csv can be found in data directory, if not we extract it from corresponding zip source\n",
    "for csv_file_name in csv_file_names:\n",
    "    if not os.path.exists(f\"{data_dir}/{csv_file_name}\"):\n",
    "        # if it is not found\n",
    "        zip_source_file_name = csv_to_zip_source[csv_file_name]\n",
    "        print(f\"❗'{csv_file_name}' does not exist in the '{data_dir}' directory, extracting it from zip file '{zip_source_file_name}'...\")\n",
    "        with ZipFile(zip_source_file_name, 'r') as zip:\n",
    "            zip.extract(csv_file_name, path=data_dir)\n",
    "            print(f\"Done extracting {csv_file_name} from {zip_source_file_name}\")\n",
    "    else:\n",
    "        # csv exists and found, let user know\n",
    "        print(f\"'{data_dir}/{csv_file_name}' exists ✅\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20653cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(f\"{data_dir}/Netflix_Dataset_Movie.csv\")\n",
    "ratings = pd.read_csv(f\"{data_dir}/Netflix_Dataset_Rating.csv\")\n",
    "credits = pd.read_csv(f\"{data_dir}/tmdb_5000_credits.csv\")\n",
    "tmdb_movies = pd.read_csv(f\"{data_dir}/tmdb_5000_movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97cc3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose only released movies\n",
    "released_movies = tmdb_movies[tmdb_movies.status == 'Released']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d8e676",
   "metadata": {},
   "source": [
    "The movies will now be given unique IDs, because it turned out, that sometimes the same movie had different IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d96eb654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find unique movies for tmdb movies\n",
    "tmdb_uniqueMovies = released_movies.groupby('title').count().reset_index()\n",
    "tmdb_uniqueMovies['Movie_ID'] = np.arange(0,len(tmdb_uniqueMovies)) #add unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef063aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new ID\n",
    "credits = pd.merge(tmdb_uniqueMovies[['title', 'Movie_ID']], credits, on = \"title\")\n",
    "#delete old ID\n",
    "credits = credits.drop('movie_id', axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5233ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new ID\n",
    "df_tmdb = pd.merge(tmdb_uniqueMovies[['title', 'Movie_ID']], released_movies, on = \"title\")\n",
    "#delete old ID\n",
    "df_tmdb = df_tmdb.drop('id', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec660358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings and movies merged\n",
    "rating_merged = pd.merge(movies, ratings, on = \"Movie_ID\")\n",
    "df_movieID = rating_merged.groupby('Name').count().reset_index().reset_index() #create new index(one for each movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8f6beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the 2 dataframe(with only the relevant attributes)\n",
    "rating_merged_new = pd.merge(df_movieID[['index', 'Name']], rating_merged[['Year', 'Name', 'User_ID', 'Rating']], on = 'Name')\n",
    "#rename index to Movie_ID\n",
    "rating_merged_new = rating_merged_new.rename(columns={'index': 'Movie_ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3281783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split rating_merged_new into 2 df - now with one unique ID for each movie\n",
    "ratings = rating_merged_new[['User_ID', 'Rating', 'Movie_ID']]\n",
    "movies = rating_merged_new[['Movie_ID', 'Year', 'Name']].drop_duplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3060a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose movies from netflix dataset that is in the tmdb dataset and is rated\n",
    "movies_subset = movies[(movies.Name.isin(df_tmdb.title)) & (movies.Movie_ID.isin(ratings.Movie_ID))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove movies, where there are either/or not genres, keywords or tagline\n",
    "df_tmdb = df_tmdb[df_tmdb.genres != '[]']\n",
    "df_tmdb = df_tmdb[df_tmdb.keywords != '[]']\n",
    "df_tmdb = df_tmdb[df_tmdb.tagline.notnull()].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab659fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove movies not in df_tmdb(after some were removed)\n",
    "df_movies = movies_subset[movies_subset.Name.isin(df_tmdb.title)]\n",
    "df_ratings = ratings[ratings.Movie_ID.isin(df_movies.Movie_ID)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf72cc",
   "metadata": {},
   "source": [
    "Now a subset of the data is chosen, such that all rated movies from netflix, that is also in the TMDB dataset, is chosen. Furthermore, we choose 1620 movies that were not rated, as new movies for the movie database. In that way we can also recommend movies not yet rated by any user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af4132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_movies = df_tmdb[~df_tmdb.title.isin(df_movies.Name)] #movies not rated\n",
    "df_tmdb = pd.concat([new_movies.sample(n=1620, random_state=20), df_tmdb[df_tmdb.title.isin(df_movies.Name)]]).reset_index()\n",
    "df_credits = credits[credits.title.isin(df_tmdb.title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batman with different cast\n",
    "df_credits[df_credits.title == 'Batman']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e1cb7",
   "metadata": {},
   "source": [
    "The movies now have a unique ID. However, it was found that at least one movie has the same title, but different cast. This has now the same movie ID, although it might not be correct. However, the system only recommends a title of a movie, and not an ID. Therefore, we chose to keep it like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert from string to list\n",
    "df_tmdb['genres'] = df_tmdb['genres'].apply(json.loads)\n",
    "df_tmdb['keywords'] = df_tmdb['keywords'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertJSON(df, column): #column as string\n",
    "    columnDict = {}\n",
    "    for i in range(0,len(df)): #loop through index\n",
    "        columnList = []\n",
    "        for j in range(0, len(df[column][i])): #loop through column for that row\n",
    "            columnList.append(df[column][i][j]['name'])\n",
    "        columnDict[i] = ', '.join(columnList)\n",
    "        \n",
    "    return columnDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68743ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert from json \n",
    "genreDict = convertJSON(df_tmdb, 'genres')\n",
    "keywordsDict = convertJSON(df_tmdb, 'keywords')\n",
    "\n",
    "df_tmdb['genres'] = genreDict.values()\n",
    "df_tmdb['keywords'] = keywordsDict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83506307",
   "metadata": {},
   "source": [
    "# **1. Find preferred genre**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55767af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterByGenre(movieTitle, df): #find movies with at least one of the same genres as the movie just seen\n",
    "    genres = df[df.title == movieTitle].genres.str.replace(' ', '|').iloc[0] ##replace , by |. This is the genres to look for in \n",
    "    genreMovies = df[df.genres.str.contains(genres)] #find movies with at least one of the genres\n",
    "    \n",
    "    return genreMovies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d78f2a",
   "metadata": {},
   "source": [
    "# **2. Movie network**\n",
    "---\n",
    "The network analysis investigates the relationship between movies through their cast. The network will be an undirected network where:\n",
    "\n",
    "- **Nodes**: Movies\n",
    "- **Edges**: Two movies are connected if they have a common actor in their top 5 staring actors.\n",
    "\n",
    " The network will consists of nodes which will represent movies. A pair of movies will have an edge between them if they have at least one common actor of their top five actors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736e350",
   "metadata": {},
   "source": [
    "## **Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_movies = nx.Graph()\n",
    "\n",
    "# Iterate over each row in the credits dataframe\n",
    "for index, row in df_credits.iterrows():\n",
    "    cast = json.loads(row['cast']) # Parse the cast column (=JSON string)\n",
    "    \n",
    "    # Get names of the first 5 actors\n",
    "    actors = [actor['name'] for actor in cast[:5]]\n",
    "    G_movies.add_node(row['title'], actors=actors)\n",
    "\n",
    "# Iterate over all pairs of movies\n",
    "for movie1, movie2 in combinations(G_movies.nodes, 2):\n",
    "    # Actors for each movie\n",
    "    actors1 = G_movies.nodes[movie1]['actors']\n",
    "    actors2 = G_movies.nodes[movie2]['actors']\n",
    "    common_actors = len(set(actors1) & set(actors2)) # Number of common actors\n",
    "    \n",
    "    # If they have at least 1 common actors, add an edge between them\n",
    "    if common_actors >= 1:\n",
    "        G_movies.add_edge(movie1, movie2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b803a",
   "metadata": {},
   "source": [
    "### **Get largest connected component**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ec170",
   "metadata": {},
   "source": [
    "Some nodes may not be connected and to avoid communities with only one movie, the focus is solely on the largest connected component for the partitioning into community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09986c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largest connected component\n",
    "largest_cc = max(nx.connected_components(G_movies), key=len)\n",
    "G_movies_lcc = G_movies.subgraph(largest_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25f3ad",
   "metadata": {},
   "source": [
    "### **Plot of network (largest connected component)**\n",
    "(This is not a part of the report but gives an overall overview of the visualized network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes approximately 10 seconds to run\n",
    "# Layout\n",
    "plt.figure(figsize=(15, 7))\n",
    "pos = nx.spring_layout(G_movies_lcc, k=0.7)\n",
    "\n",
    "# Draw network\n",
    "nx.draw(G_movies_lcc, pos, node_color=d_color, edge_color='black', with_labels=False, node_size=30, width=0.1)\n",
    "\n",
    "# Legend\n",
    "movies_patch = mpatches.Patch(color=d_color, label='Movies')\n",
    "plt.legend(handles=[movies_patch])\n",
    "plt.title(\"Network of Movies (largest connected component)\", fontsize=30, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f55b19",
   "metadata": {},
   "source": [
    "### **Nodes and edges**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786963ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the whole network\n",
    "n_nodes = len(G_movies.nodes())\n",
    "n_edges = len(G_movies.edges())\n",
    "print(\"For the whole network\")\n",
    "print(\"\\t - Number of nodes:\", n_nodes)\n",
    "print(\"\\t - Number of edges:\", n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the network consisting only of the largest connected component\n",
    "n_nodes2 = len(G_movies_lcc.nodes())\n",
    "n_edges2 = len(G_movies_lcc.edges())\n",
    "print(\"For the network consisting only of the largest connected component\")\n",
    "print(\"\\t - Number of nodes:\", n_nodes2)\n",
    "print(\"\\t - Number of edges:\", n_edges2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92f994",
   "metadata": {},
   "source": [
    "## **Communities**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f98324",
   "metadata": {},
   "source": [
    "In the following when identifying the communities, a high modularity is the main objective. The course introduces the *Girvan-Newman algorithm*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5268cc",
   "metadata": {},
   "source": [
    "### **Girvan-Newman algorithm**\n",
    "\n",
    "<span style = \"color:red\"> Warning: This takes approximately 2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fc967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girvan–Newman algorithm\n",
    "nx_communities = nx.algorithms.centrality.edge_betweenness_centrality(G_movies_lcc)\n",
    "nx_girvan_newman = nx.algorithms.community.centrality.girvan_newman(G_movies_lcc)\n",
    "nx_best_community = tuple(sorted(c) for c in next(nx_girvan_newman))\n",
    "num_communities = len(nx_best_community)\n",
    "nx_modularity_G = nx.algorithms.community.quality.modularity(G_movies_lcc, nx_best_community)\n",
    "\n",
    "print(\"Girvan–Newman:\")\n",
    "print(\"\\t- Number of communities found by Girvan-Newman:\", num_communities)\n",
    "print(\"\\t- Modularity:\", round(nx_modularity_G,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd0701b",
   "metadata": {},
   "source": [
    "When using the *Girvan-Newman algorithm* the algorithm identifies to communities and the modularity is $0.0007$. The modularity is $0.0007 << 0.3$ and the partitioning of the communities is therefore not considered good at all. The course also introduces *Spectral clustering*, which will be used to get a better partitioning and therefore modularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29625ca7",
   "metadata": {},
   "source": [
    "### **Spectral clustering**\n",
    "In the following spectral clustering is used to partition the network into communities. This is done by using Python's `sklearn.cluster.SpectralClustering` to find the best modularity for an interval of communities from 2 to 99. \n",
    "\n",
    "<span style = \"color:red\"> Warning: This takes approximately 7 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral clustering - Finding the best number of clusters\n",
    "np.random.seed(seed)\n",
    "\n",
    "adj_mat = nx.to_numpy_array(G_movies_lcc)\n",
    "nodes = list(G_movies_lcc.nodes())\n",
    "\n",
    "best_modularity = -np.inf\n",
    "best_n_clusters = None\n",
    "best_communities = None\n",
    "\n",
    "# Try different numbers of clusters\n",
    "for n_clusters in range(2, 100):\n",
    "    sc = SpectralClustering(n_clusters, affinity='precomputed', n_init=100)\n",
    "    sc.fit(adj_mat)\n",
    "\n",
    "    # Convert labels array to list of sets format\n",
    "    communities = [set(nodes[i] for i in np.where(sc.labels_ == j)[0]) for j in range(n_clusters)]\n",
    "    modularity = nx.algorithms.community.quality.modularity(G_movies_lcc, communities)\n",
    "\n",
    "    # Update\n",
    "    if modularity > best_modularity:\n",
    "        best_modularity = modularity\n",
    "        best_n_clusters = n_clusters\n",
    "        best_communities = communities\n",
    "\n",
    "print(\"Best modularity is\", round(best_modularity,2), \"found with\", best_n_clusters, \"clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e20ec06",
   "metadata": {},
   "source": [
    "It can be seen that the modularity is now $0.36$ with $66$ communities, however the computational time to calculate this is very long. Therefore, we will try to use another algorithm which is the *Louvain algorithm*. The Louvain algorithm is essentially an algorithm that optimizes the modularity. It is also known for its computational complexity of $O(L)$, which means that it can identify communities in networks with a large number of nodes (This is what we want).\n",
    "- Source: http://networksciencebook.com/chapter/9 (Section 9.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045e89c",
   "metadata": {},
   "source": [
    "### **Louvain algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6173e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Louvain algorithm\n",
    "communities_Louvain = cl.best_partition(G_movies_lcc, random_state=seed)\n",
    "n_communities_Louvain = len(np.unique(list(communities_Louvain.values())))\n",
    "modularity_Louvain = community.modularity(communities_Louvain, G_movies_lcc)\n",
    "\n",
    "print(\"Louvain:\")\n",
    "print(\"\\t- Number of communities found by Louvain:\", n_communities_Louvain)\n",
    "print(\"\\t- Modularity:\", round(modularity_Louvain,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9ead9",
   "metadata": {},
   "source": [
    "The modularity for the Louvain algorithm is $0.42$ and it found $20$ communities. The modularity is now $0.42 > 0.3$ and therfore considered a good partitioning. From the three different methods to identify communities, the Louvain algorithm shows the best result and the following calculations will therefore be based on the Louvain algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a5e93",
   "metadata": {},
   "source": [
    "## **Bar plot of the number of movies in each community and outside the largest connected component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425bc3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of movies in each community\n",
    "communities_Louvain_list = [set() for _ in range(n_communities_Louvain)]\n",
    "for node, community_id in communities_Louvain.items():\n",
    "    communities_Louvain_list[community_id].add(node)\n",
    "\n",
    "num_movies = [len(community) for community in communities_Louvain_list]\n",
    "\n",
    "# Calculating nodes that is not in the largest connected component\n",
    "all_nodes = set(G_movies.nodes())\n",
    "lcc_nodes = set(G_movies_lcc.nodes())\n",
    "non_lcc_nodes = all_nodes - lcc_nodes\n",
    "num_non_lcc_movies = len(non_lcc_nodes)\n",
    "num_movies.append(0) # Add an empty bar before the last bar (Just to seperate the last bar)\n",
    "num_movies.append(num_non_lcc_movies) # Add the number to num_movies\n",
    "\n",
    "# Colors\n",
    "cmap = plt.cm.get_cmap(network_color_map, n_communities_Louvain + 2)\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(12, 5))\n",
    "bars = plt.bar(range(1, n_communities_Louvain + 3), num_movies, color=cmap(range(n_communities_Louvain + 2)))\n",
    "\n",
    "# Create a list of labels for the x-axis\n",
    "xtick_labels = [str(i) for i in range(1, n_communities_Louvain + 1)] + [\"\", \"Movies not \\n in a community\"]\n",
    "plt.xticks(range(1, n_communities_Louvain + 3), xtick_labels)\n",
    "\n",
    "plt.ylim(0, max(num_movies) + (max(num_movies)/10))\n",
    "plt.xlabel(\"Community\")\n",
    "plt.ylabel(\"Number of movies\")\n",
    "plt.title(\"Number of Movies in Each Community and Outside the Largest Connected Component\", fontweight=\"bold\")\n",
    "\n",
    "# Add the precise number of movies above each bar\n",
    "for i, bar in enumerate(bars):\n",
    "    bar_height = bar.get_height()\n",
    "    if bar_height != 0:  # Skip the empty bar\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar_height + 0.5, bar_height, ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aef7f3",
   "metadata": {},
   "source": [
    "The plot shows the $20$ communities and the number of movies in each community. Additionally, the last bar shows that there are in total $151$ movies that are not in a community. Even though they are not in a community, the movies will still be kept. This is explained further in a section below called *\"Function to return a dataframe of the community of the input movie\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966db811",
   "metadata": {},
   "source": [
    "## **Plot of the network partitioned into communities of movies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_new = plt.cm.get_cmap(network_color_map, len(communities_Louvain_list) + 2)\n",
    "community_to_color = {i: cmap_new(i) for i in range(len(communities_Louvain_list) + 2)}\n",
    "color_map = {node: community_to_color[communities_Louvain[node]] for node in G_movies_lcc.nodes if communities_Louvain[node] in community_to_color}\n",
    "seed = 1\n",
    "\n",
    "# To determine the brightness of a node to see if its label should be black or white\n",
    "def brightness(color):\n",
    "    # Based on https://stackoverflow.com/questions/596216/formula-to-determine-perceived-brightness-of-rgb-color\n",
    "    rgb = mcolors.colorConverter.to_rgb(color)\n",
    "    return 0.299*rgb[0] + 0.587*rgb[1] + 0.114*rgb[2]\n",
    "\n",
    "# To darken colors of the edges between two nodes from the same community\n",
    "def darken_color(color, factor=0.8):\n",
    "    rgb = mcolors.colorConverter.to_rgb(color)\n",
    "    hsv = colorsys.rgb_to_hsv(*rgb)\n",
    "    hsv_darker = hsv[0], hsv[1], hsv[2] * factor\n",
    "    rgb_darker = colorsys.hsv_to_rgb(*hsv_darker)\n",
    "    return rgb_darker\n",
    "\n",
    "def community_layout(g, partition, seed=None):\n",
    "    pos_communities = position_communities(g, partition, seed=seed, scale=30)\n",
    "    pos_nodes = position_nodes(g, partition, seed=seed, scale=7.5)\n",
    "\n",
    "    # combine positions\n",
    "    pos = dict()\n",
    "    for node in g.nodes():\n",
    "        pos[node] = pos_communities[node] + pos_nodes[node]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def position_communities(g, partition, seed=None, **kwargs):\n",
    "    # create a weighted graph, in which each node corresponds to a community,\n",
    "    # and each edge weight to the number of edges between communities\n",
    "    between_community_edges = find_between_community_edges(g, partition)\n",
    "\n",
    "    communities = set(partition.values())\n",
    "    hypergraph = nx.DiGraph()\n",
    "    hypergraph.add_nodes_from(communities)\n",
    "    for (ci, cj), edges in between_community_edges.items():\n",
    "        hypergraph.add_edge(ci, cj, weight=len(edges))\n",
    "\n",
    "    # Layout for communities\n",
    "    pos_communities = nx.spring_layout(hypergraph, seed=seed, k=10, **kwargs)\n",
    "\n",
    "    # set node positions to position of community\n",
    "    pos = dict()\n",
    "    for node, community in partition.items():\n",
    "        pos[node] = pos_communities[community]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def find_between_community_edges(g, partition):\n",
    "    edges = dict()\n",
    "\n",
    "    for (ni, nj) in g.edges():\n",
    "        ci = partition[ni]\n",
    "        cj = partition[nj]\n",
    "\n",
    "        if ci != cj:\n",
    "            try:\n",
    "                edges[(ci, cj)] += [(ni, nj)]\n",
    "            except KeyError:\n",
    "                edges[(ci, cj)] = [(ni, nj)]\n",
    "\n",
    "    return edges\n",
    "\n",
    "def position_nodes(g, partition, seed=None, **kwargs):\n",
    "    communities = dict()\n",
    "    for node, community in partition.items():\n",
    "        try:\n",
    "            communities[community] += [node]\n",
    "        except KeyError:\n",
    "            communities[community] = [node]\n",
    "\n",
    "    pos = dict()\n",
    "    for _, nodes in communities.items():\n",
    "        subgraph = g.subgraph(nodes)\n",
    "        \n",
    "        # Layout\n",
    "        pos_subgraph = nx.spring_layout(subgraph, seed=seed, k=3, **kwargs)\n",
    "        pos.update(pos_subgraph)\n",
    "\n",
    "    return pos\n",
    "\n",
    "# Node labels \n",
    "labels = {node: communities_Louvain[node] + 1 for node in G_movies_lcc.nodes}\n",
    "label_colors = {node: \"white\" if brightness(color_map.get(node, \"grey\")) < 0.5 else \"black\" for node in G_movies_lcc.nodes}\n",
    "pos = community_layout(G_movies_lcc, communities_Louvain, seed=seed)\n",
    "\n",
    "\n",
    "# Legend\n",
    "patches = [mpatches.Patch(color=community_to_color[i], label=f\"Community {i+1} ({len(communities_Louvain_list[i])} nodes)\") for i in range(len(communities_Louvain_list))]\n",
    "\n",
    "# Edges\n",
    "edge_colors = [darken_color(color_map.get(n1, \"grey\")) if communities_Louvain[n1] == communities_Louvain[n2] else \"grey\" for n1, n2 in G_movies_lcc.edges()]\n",
    "edge_widths = [0.8 if communities_Louvain[n1] == communities_Louvain[n2] else 0.15 for n1, n2 in G_movies_lcc.edges()]\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 12))\n",
    "nx.draw(G_movies_lcc, pos, node_color=[color_map.get(node, \"grey\") for node in G_movies_lcc.nodes], edge_color=edge_colors, width=edge_widths, with_labels=False, node_size=240)\n",
    "\n",
    "for node, (x, y) in pos.items():\n",
    "    plt.text(x, y, labels[node], fontsize=10, color=label_colors[node], ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.legend(handles=patches)\n",
    "plt.title(\"Network Partitioned Into Communities of Movies\", fontsize=30, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77aea8",
   "metadata": {},
   "source": [
    "The plot above shows a plot of the different communities in the network. (*The report comments on this further*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749b46e",
   "metadata": {},
   "source": [
    "## **Function to return a dataframe of the community of the input movie**\n",
    "The following function will be used in the final recommendation system. The function takes a movie title and a dataframe of movies. Based on these parameters, it will find the community which the input movie is a part of and return a dataframe with only the movies from that community. If the input movie is not a part of a community then the function will return the input dataframe. This means that the recommendation system will skip the network analysis if the input movie is not a part of a community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_community_df(movie_title, df):\n",
    "    # Find community of the given movie\n",
    "    for community in communities_Louvain_list:\n",
    "        if movie_title in community:\n",
    "            df_tmdb_community = df[df['title'].isin(community)] # Filter dataframe so it only includes the movies in the community of the input movie           \n",
    "            return df_tmdb_community\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a60c15",
   "metadata": {},
   "source": [
    "---\n",
    "# **Topic Modeling**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c4550",
   "metadata": {},
   "source": [
    "\n",
    "## **Text preprocessing**\n",
    "Before performing Topic Modeling, we need to preprocess the data. For the preprocessing, the function `preProcess`, is created. This function make text to lowercase, remove stop words, remove punctuation and lemmatize the text. Stopwords will be removed before removing punctuation, which is important, since some words will no longer be a stop word, if punctuation is removed before stop words. Lemmatization is used instead of stemming because lemmatization uses lexical knowledge base to get base form of words instead of cutting the last part of the [word](https://medium.com/product-ai/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908).\n",
    "\n",
    "As the last step, a dictionary and corpus is created from the text. The dictionary is a dictionary with all the words that are in the text about the movies. The corpus is a list of lists. It consists of a list for each movie, which consists of a number of tuples. Each tuple consists of a word ID, the key for the word from the dictionary, and a number showing the number of times the word is in the text about the movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15411f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(string):\n",
    "    new_string = []\n",
    "    stopWords = stopwords.words('english')\n",
    "    \n",
    "    for s in string.split():\n",
    "        if s not in stopWords:\n",
    "            new_string.append(s)\n",
    "    final_string = ' '.join(new_string)\n",
    "    \n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d514fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(string, regex):\n",
    "    result = re.sub(regex, '', string)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(string): #remove: . , ! ? ' ’ : \" ( ) $ - and numbers\n",
    "    reg = [r'\\.', r'\\,', r'\\?', r'\\'', r'’', r'\\\"', r':', r':', r'\\(', r'\\)', r'\\$', r'-', r'\\d'] \n",
    "\n",
    "    for r in reg:\n",
    "        string = remove(string, r)\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b1bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    for w in string.split():\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(w))\n",
    "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(df, column): #function for all preprocess\n",
    "    lower = df[column].str.lower() #lowercase\n",
    "    removeStopW = lower.apply(lambda x: removeStopWords(x)) #remove stop words\n",
    "    removePunct = removeStopW.apply(lambda x: removePunctuation(x)) #remove punctuation\n",
    "    df[column] = removePunct.apply(lambda x: lemmatize(x)) #lemmatize\n",
    " \n",
    "    return df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42541d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDocs(df, column): #create documents for movies\n",
    "    text_list = ','.join(df[column]).split(',') #get all text into a list. Each document/movie is splitted by comma \n",
    "    text = [d.split() for d in text_list] #get a lis of list, where each list is a document/movie. Each word is as individually string. get each string into array.\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b6410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDictCorpus(text): #create dictionary and corpus\n",
    "    #create dictionary\n",
    "    dictionary = corpora.Dictionary(text) #create dictionary\n",
    "    \n",
    "    #create corpus\n",
    "    corpus = [dictionary.doc2bow(t) for t in text]\n",
    "    \n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess\n",
    "movie_text = pd.DataFrame()\n",
    "movie_text['title'] = df_tmdb['title']\n",
    "movie_text['overview'] = preProcess(df_tmdb, 'overview')\n",
    "movie_text['tagline'] = preProcess(df_tmdb, 'tagline')\n",
    "movie_text['genres'] = preProcess(df_tmdb, 'genres')\n",
    "movie_text['keywords'] = preProcess(df_tmdb, 'keywords')\n",
    "\n",
    "ind = movie_text[movie_text.tagline != ''].index #index of movie without tagline\n",
    "movie_text = movie_text.iloc[ind].reset_index()\n",
    "\n",
    "#create text\n",
    "overviewText = createDocs(movie_text, 'overview')\n",
    "taglineText = createDocs(movie_text, 'tagline')\n",
    "genresText = createDocs(movie_text, 'genres')\n",
    "keywordsText = createDocs(movie_text, 'keywords')\n",
    "\n",
    "#create dictionary and corpus\n",
    "overviewDict, overviewCorpus = getDictCorpus(overviewText)\n",
    "taglineDict, taglineCorpus = getDictCorpus(taglineText)\n",
    "genresDict, genresCorpus = getDictCorpus(genresText)\n",
    "keywordsDict, keywordsCorpus = getDictCorpus(keywordsText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb4776",
   "metadata": {},
   "source": [
    "## **TF-IDF**\n",
    "After the preprocessing were performed, we started looking at TF-IDF. This shows the words that is most important/tells most about the text of the movie. This means, that it will not show the words that are often in a lot of the text about movies. There are several kinds of TF-IDF but the one performed here is the one taught in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(corpus, dictionary, documents):\n",
    "    N = len(corpus) #number of documents\n",
    "    f = np.zeros((len(documents), len(dictionary)))#matrix in size: number of documents x number of unique words in all documents\n",
    "    tf = np.zeros((len(documents), len(dictionary)))#matrix in size: number of documents x number of unique words in all documents\n",
    "    n_t = np.zeros(len(dictionary)) #array for counting number of documents word t appears in\n",
    "    \n",
    "    #for d in documents: #loop through all documents\n",
    "    for d in range(0,len(documents)):\n",
    "        for key in dictionary.keys(): #loop through all words\n",
    "            word = dictionary[key]\n",
    "            wordCount = documents[d].count(word) #count occurence of word in each document\n",
    "            f[d, key] = wordCount\n",
    "            \n",
    "            if wordCount >= 1: #if word is in document, add 1 to document count\n",
    "                n_t[key] += 1 \n",
    "            \n",
    "        tf[d,:] = f[d,:]/max(f[d,:]) \n",
    "    \n",
    "    idf = np.log2((N/n_t))\n",
    "    TF_IDF = tf * idf\n",
    "   \n",
    "    return TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b91ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_overview = tf_idf(overviewCorpus, overviewDict, overviewText)\n",
    "TF_IDF_tagline = tf_idf(taglineCorpus, taglineDict, taglineText)\n",
    "TF_IDF_genres = tf_idf(genresCorpus, genresDict, genresText)\n",
    "TF_IDF_keywords = tf_idf(keywordsCorpus, keywordsDict, keywordsText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b398e",
   "metadata": {},
   "source": [
    "## **Wordcloud**\n",
    "After performing TF-IDF, we take a look at woudclouds for overview, tagline, genres and keywords. The wordclouds are shown with and without TF-IDF, and are shown for all data(after removing movies with missing data for those four text attributes). It seems that for overview, tagline and genre the wordclouds with and without TF-IDF seems quite similar. Since TF-IDF is great for finding topics, we chose to use the 10 most important words in a text(according to TF-IDF) for the topic modeling. We hope this will give us the topics that tells most about the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eda561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCloud(df, column): #create wordcloud for plotting\n",
    "    text = ' '.join(df[column])\n",
    "    \n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e6d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_overview = wordCloud(movie_text, 'overview')\n",
    "wordcloud_tagline = wordCloud(movie_text, 'tagline')\n",
    "wordcloud_genre_text = wordCloud(movie_text, 'genres')\n",
    "wordcloud_keywords_text = wordCloud(movie_text, 'keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot wordcloud for preprocessed text(wihtout TF-IDF)\n",
    "plt.subplots(nrows=2, ncols=2)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(wordcloud_overview, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Overview\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(wordcloud_tagline, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Tagline\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(wordcloud_genre_text, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Genre\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(wordcloud_keywords_text, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Keywords\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(TF_IDF_array, dictionary):\n",
    "    frequency_dict = {}\n",
    "    for i in range(0, len(dictionary)):\n",
    "        frequency_dict[dictionary[i]] = np.sum(TF_IDF_array[:,i])\n",
    "    return frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dict with frequency of TF-IDF\n",
    "overviewFrequency = frequency(TF_IDF_overview, overviewDict)\n",
    "taglineFrequency = frequency(TF_IDF_tagline, taglineDict)\n",
    "genreFrequency = frequency(TF_IDF_genres, genresDict)\n",
    "keywordsFrequency = frequency(TF_IDF_keywords, keywordsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create wordclouds for TF-IDF\n",
    "overviewWordcloud = WordCloud().generate_from_frequencies(frequencies = overviewFrequency)\n",
    "taglineWordcloud = WordCloud().generate_from_frequencies(frequencies = taglineFrequency)\n",
    "genreWordcloud = WordCloud().generate_from_frequencies(frequencies = genreFrequency)\n",
    "keywordsWordcloud = WordCloud().generate_from_frequencies(frequencies = keywordsFrequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e34e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot wordcloud for preprocessed text(with frequency as weight based on TF-IDF)\n",
    "plt.subplots(nrows=2, ncols=2)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(overviewWordcloud, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Overview\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(taglineWordcloud, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Tagline\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(genreWordcloud, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Genre\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(keywordsWordcloud, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Keywords\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImportantWords(tf_idf, corpus, dictionary): #find 10 most important words based on TF-IDF\n",
    "    important_words = []\n",
    "    for d in range(0,len(corpus)):\n",
    "        index_largest = tf_idf[d,:].argsort()[-10:]\n",
    "        important_words.append([dictionary[i] for i in index_largest])\n",
    "        \n",
    "    return important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 10 most important words for each movie\n",
    "importantWords_overview = getImportantWords(TF_IDF_overview, overviewCorpus, overviewDict)\n",
    "importantWords_tagline = getImportantWords(TF_IDF_tagline, taglineCorpus, taglineDict)\n",
    "importantWords_genre = getImportantWords(TF_IDF_genres, genresCorpus, genresDict)\n",
    "importantWords_keywords = getImportantWords(TF_IDF_keywords, keywordsCorpus, keywordsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a7034",
   "metadata": {},
   "source": [
    "## **LDA**\n",
    "Now for the actual topic modeling, which is performed on the 10 most important words for each text/movies, according to TF-IDF. The reason for this is that TF-IDF is a great way to find topics, so by using that for the topic modeling, we hope to get the best topics. Initially the topic modeling was performed with and without TF-IDF and with genres, tagline, overview and keywords. However, we ended up only doing topic modeling on overview and keywords. The reason for not performing topic modeling on genre is that genres is already a type of topic, that movies are grouped into. Therefore we chose to use the genres in the beginning of the recommender system instead. \n",
    "The reason for not performing topic modeling on the tagline is that it didn't really explain much about the movies, so we didn't think it was a good attribute to base a recommendation on. \n",
    "\n",
    "\n",
    "The topic modeling is performed using Latent Dirichlet Allocation(LDA). The implementation is performed with inspiration from [this](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0) article. LDA is an unsupervised probabilistic algorithm that finds topics in a set of documents based on the words in the documents. \n",
    "\n",
    "To perform LDA, we need to know the number of topics. There are different ways to investigate this, but here we have chosen to try different number of topics, and check how the topics look, and how it looks when plotted. Besides that, we don't want only a few movies in the same topic, because we want to find movies within the same topics, as a specific movie. But before recommending movies, we want to find similar users, and base our final recommendation on which movies the similar users liked. Therefore, we need a certain amount of movies in each topic, otherwise we won't have any movies left to recommend. The topic modeling is performed if there are at least 15 movies left to recommend. Then the number of topics is chosen to be $\\frac{\\text{# movies}}{8}$, such that there on average is 8 movies in each topic. \n",
    "\n",
    "The algorithm can assign several topics to a text but will return the probability of the topic belonging to that text. Therefore, we assigned the topic with the highest probability, so we have one topic per text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c69642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(n_topics, corpus, dictionary): #create function for LDA\n",
    "    lda = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                       id2word=dictionary,\n",
    "                                       num_topics=n_topics, \n",
    "                                     random_state= 5)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_plot(path, lda_model, corpus, dictionary, n_topics): #function for plotting topics\n",
    "    #Visualize topics\n",
    "    pyLDAvis.enable_notebook()\n",
    "    LDAvis_data_filepath = os.path.join(path + str(n_topics))\n",
    "    \n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "            \n",
    "    with open(LDAvis_data_filepath, 'rb') as f:\n",
    "        LDAvis_prepared = pickle.load(f)\n",
    "        \n",
    "    pyLDAvis.save_html(LDAvis_prepared, path+ str(n_topics) +'.html')\n",
    "    \n",
    "    return LDAvis_prepared "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignTopic(lda, text, corpus): #assign topic with highest score\n",
    "    assignedTopic = []\n",
    "    for t in lda[corpus]: #loop through topics\n",
    "        topic = []\n",
    "        score = []\n",
    "        for i in t:\n",
    "            topic.append(i[0])\n",
    "            score.append(i[1])\n",
    "        assignedTopic.append(topic[np.argmax(score)]) #assign topic with highest score\n",
    "    return assignedTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TopicModeling_preprocessing(df, column):\n",
    "    #create preProcess df\n",
    "    df[column] = preProcess(df, column)\n",
    "    \n",
    "    #create dictionary, corpus and text for overview from preprocessed data\n",
    "    colText = createDocs(df, column)\n",
    "    colDict, colCorpus = getDictCorpus(colText)\n",
    "    \n",
    "    #TF-IDF\n",
    "    TF_IDF = tf_idf(colCorpus, colDict, colText)\n",
    "   \n",
    "    #10 most important words - get text\n",
    "    text = getImportantWords(TF_IDF, colCorpus, colDict)\n",
    "    #dict and corpus for 10 most important words\n",
    "    dictionary, corpus = getDictCorpus(text)\n",
    "    \n",
    "    return dictionary, corpus, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TopicModeling(df, movieTitle):\n",
    "    if len(df) >= 15: #do topic modeling\n",
    "        # get dictionary, corpus and text, 10 most important words\n",
    "        overviewDict, overviewCorpus, overviewText = TopicModeling_preprocessing(df, 'overview')\n",
    "        keywordsDict, keywordsCorpus, keywordsText = TopicModeling_preprocessing(df, 'keywords')\n",
    "        n_topics = int(np.floor(len(df)/8)) #number of topics is assigned such that there on average is 6 movies per topic for overview\n",
    "        \n",
    "        #create LDA\n",
    "        overviewLDA = LDA(n_topics, overviewCorpus, overviewDict)\n",
    "        keywordsLDA = LDA(n_topics, keywordsCorpus, keywordsDict)\n",
    "    \n",
    "        #get columns for topics\n",
    "        overviewTopic = assignTopic(overviewLDA, overviewText, overviewCorpus)\n",
    "        keywordsTopic = assignTopic(keywordsLDA, keywordsText, keywordsCorpus) \n",
    "        #add overview topic to df\n",
    "        df['overview_topic'] = overviewTopic \n",
    "        df['keywords_topic'] = keywordsTopic \n",
    "        \n",
    "        #Topics for the seen movie\n",
    "        movie_overviewTopic = df[df.title == movieTitle].overview_topic.iloc[0]\n",
    "        movie_keywordsTopic = df[df.title == movieTitle].keywords_topic.iloc[0]\n",
    "        \n",
    "    \n",
    "        #find movies where movies have at least one topic(overview or keywords) as the movieTitle\n",
    "        TP_recommend = df[(df.overview_topic == movie_overviewTopic) | (df.keywords_topic == movie_keywordsTopic)]\n",
    "            \n",
    "    else:\n",
    "        TP_recommend = df\n",
    "    \n",
    "    recommendedMovies = TP_recommend[TP_recommend.title != movieTitle] #remove seen movie\n",
    "    \n",
    "    return recommendedMovies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a6b99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Selecting similar users (under Book chapter 9.3/Normalizing ratings and Example 9.9)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d4dfd",
   "metadata": {},
   "source": [
    "To calculate the how similar two users are in terms of ratings, we use cosine distance measurments. For this, we go thorught the following steps:\n",
    "* Define a utility matrix, representing how each user rated each movie with what rating,\n",
    "* Determine the top 1000 users who rated the most movies (if have not been before),\n",
    "* Normalize the matrix, where we subtract the mean of a given user's every rating from each rating they provided,\n",
    "* Finally, we evaluate the cosine distance (detailed in book's chapter \"9.3.1 Measuring Similarity\"), if the two users have at least 'nCommonMovies' number of movies rated by both of them.\n",
    "\n",
    "From these results, we deem similar users who have a cosine distance of 0, or the closest 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_1000_user(utility_mx):\n",
    "    n_ratings_df = pd.DataFrame(index=utility_mx.index, columns=[\"n_ratings\"])\n",
    "    \n",
    "    for index, row in n_ratings_df.iterrows():\n",
    "        row[\"n_ratings\"] = len(utility_mx.loc[index,:].dropna())\n",
    "    \n",
    "    n_ratings_df = n_ratings_df.sort_values(\"n_ratings\", ascending=False)\n",
    "    np.savetxt(\"top_1000_user.csv\", n_ratings_df.iloc[:1000].index, delimiter=\",\", header=\"user_id\")\n",
    "    return n_ratings_df.iloc[:1000].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05299c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarUsers(userID, nCommonMovies):\n",
    "    #Assembling utility matrix, where row represent users (by User_ID), columns are movies (by Movie_ID), and the values are the ratings given to that movie by that user\n",
    "    uti_mx = df_ratings.pivot_table(values='Rating', index='User_ID', columns='Movie_ID', aggfunc='first')\n",
    "\n",
    "    # Calculating a list of top 1000 user_ids with the most ratings\n",
    "    top_1000 = []\n",
    "    try:\n",
    "        top_1000 = np.genfromtxt('top_1000_user.csv', delimiter=',')\n",
    "        if len(top_1000) == 0:\n",
    "            top_1000 = calculate_top_1000_user(uti_mx)\n",
    "    except Exception as e:\n",
    "        print(\"Problem occured while loading top users, calculating it live now...\")\n",
    "        top_1000 = calculate_top_1000_user(uti_mx)\n",
    "    \n",
    "    uti_mx_top_1000 = uti_mx.loc[top_1000]\n",
    "\n",
    "    #Normalize utility matrix, subtracting mean of ratings from the actual ratings for each respective user\n",
    "    norm_uti_mx = uti_mx_top_1000.sub(uti_mx_top_1000.mean(axis=1), axis=0)\n",
    "    \n",
    "    #Calculating Cosine distance of a highlighted user and every other user if they have at least 'n_common_movies' that they both rated\n",
    "    #Number of common movies needed to be considering other user\n",
    "    \n",
    "    #init matrix\n",
    "    cosine_dist_mx = pd.DataFrame(index=[userID], columns=norm_uti_mx.index)\n",
    "    cosine_dist_mx = cosine_dist_mx.drop(userID, axis=1)\n",
    "    \n",
    "    chosen_set = set(norm_uti_mx.loc[userID,:].dropna().index)\n",
    "        \n",
    "    for user in top_1000:\n",
    "        # list of common movie IDs\n",
    "        common_list = list(chosen_set & set(norm_uti_mx.loc[user,:].dropna().index))\n",
    "        if len(common_list) > nCommonMovies:\n",
    "            a = norm_uti_mx.loc[userID, common_list].values\n",
    "            b = norm_uti_mx.loc[user, common_list].values\n",
    "\n",
    "            # If user_id rated with only one rating, normalizing made it into all 0 array, which does not tell a lot. Returning 1.0 in that case, middle of returnable range.\n",
    "            if not any(b):\n",
    "                cosine_dist_mx[user] = 1.0\n",
    "            else:            \n",
    "                cosine_dist_mx[user] = distance.cosine(a,b)\n",
    "            \n",
    "    # sort by distance, lower distance means more similar\n",
    "    cosine_dist_mx_sorted = cosine_dist_mx.sort_values(userID, axis=1).dropna(axis=1)\n",
    "    similarity = cosine_dist_mx_sorted.T\n",
    "    \n",
    "\n",
    "    mostSimilarUsers = similarity[similarity[userID] == 0][userID].keys().to_list() #find users with similarity on 0 \n",
    "    if not mostSimilarUsers: #if no users has similarity on 0\n",
    "        mostSimilarUsers = similarity[userID].nsmallest(10).keys().to_list() #userID for 10 most similar users\n",
    "    \n",
    "    return mostSimilarUsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d93bb",
   "metadata": {},
   "source": [
    "---\n",
    "# **Merging the Methods to a Recommendation System**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84beaf",
   "metadata": {},
   "source": [
    "The way the recommendation system works is by first checking if the user is new. If it is, the system recommends the 5 most popular movies, since the system don't have any data on the user yet. \n",
    "\n",
    "If the user is a exsisting user, it checks if the user liked the just seen movie. If the user did like it, it removes all movies from our movie database that the user already saw, except for the one just seen, since we need information about that movie later. The movies that are left now, is now filtered by genre. All movies with same genre as the just seen movies, are found. There are often several genres, but they just need to have one in common to be kept in the database. Now, the community for the just seen movie is found. Then, all movies left in the database, that is in the same community, is found. Then topic modeling is performed, where only movies with at least one topic in common with the liked movie, is kept. Afterwards, the most similar users are found. Based on these, the 5 movies left with higest average score is found, and recommended. \n",
    "\n",
    "If the user didn't like the movie just seen, the system finds the movie they have seen with highest score. Often there will be several movies with same rating. We would like to take the one seen latest, but since we don't have time of watching the movies, we are taking the first element with higest rating. If this rating is 3 or greater, we use the methods described above, but with this liked movie, instead of the movie they did just watch, which they didn't like. If the users didn't like any movies seen so far, the most similar users are found, and the highest rated movies by the similar users are recommended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29dde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestMovies_similarUsers(similarUsers_df, movie_df, db_movies): #return bestMovies based on similar users\n",
    "    movies = movie_df[movie_df.Name.isin(db_movies.title)]  #ID of movies to choose between \n",
    "    movieRatings = similarUsers_df[similarUsers_df.Movie_ID.isin(movies.Movie_ID)].groupby('Movie_ID').mean().reset_index() #best movies according to similar users\n",
    "    bestMovies_rating = movieRatings.sort_values(by = 'Rating', ascending = False)[0:5] #highest rated movies of similar users\n",
    "    bestMovies = movie_df[movie_df.Movie_ID.isin(bestMovies_rating.Movie_ID)].Name.to_list() #titles of best movies\n",
    "    if len(bestMovies) >= 5:\n",
    "        recommendedMovies = bestMovies\n",
    "    else:\n",
    "        missingRecommendations = 5 - len(bestMovies) #number of recommendations missing\n",
    "        popularMovies = db_movies.sort_values(by = 'popularity', ascending = False).iloc[0:missingRecommendations].title.to_list() #most popular movies\n",
    "        bestMovies += popularMovies #add populaMovies\n",
    "        recommendedMovies = bestMovies\n",
    "        \n",
    "    return recommendedMovies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4574f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation(userID, rating, movieTitle, rating_df, movie_df, TMDB_movies): #if userID is None, it is a new user\n",
    "    if userID == None: #if new user\n",
    "        #find 5 most popular movies\n",
    "        mostPopular_index = TMDB_movies.popularity.nlargest(5).index.to_list() #index of 5 most popular movies(of given day)\n",
    "        mostPopularMovies = TMDB_movies.iloc[mostPopular_index].title.to_list() #list of 5 most popular movies(of given day)\n",
    "        recommendedMovies = mostPopularMovies  #recommend most popular movies\n",
    "\n",
    "        \n",
    "    else: #if existsting user\n",
    "        userRatings = rating_df[rating_df.User_ID == userID] #rating from user\n",
    "        seenMovies_ID = userRatings.Movie_ID.to_list() #list of ID of movies already seen by user\n",
    "        seenMovies_titles = df_movies[df_movies.Movie_ID.isin(seenMovies_ID)].Name.to_list() #list of titles of movies already seen by user\n",
    "        \n",
    "        \n",
    "        if rating >= 3: #if user liked the movie\n",
    "            if movieTitle in seenMovies_titles:\n",
    "                seenMovies_titles.remove(movieTitle) #remove movieTitle from list of seen movies, since we still need that\n",
    "                \n",
    "            if seenMovies_titles: #if any seen movies, remove movies already seen\n",
    "                movies = TMDB_movies[~TMDB_movies.title.isin(seenMovies_titles)] #remove movies already seen(except the one just seen)\n",
    "            else:\n",
    "                movies = TMDB_movies\n",
    "                \n",
    "            genreFiltering = filterByGenre(movieTitle, movies) #find movies with same genre as movieTitle\n",
    "            communityMovies = get_movie_community_df(movieTitle, genreFiltering) #find movies within same community\n",
    "            TP_movies = TopicModeling(communityMovies, movieTitle) #perform topic modeling, and return movies left after that\n",
    "           \n",
    "            #find similar users \n",
    "            userSimilarity = similarUsers(userID, 20)\n",
    "            users = df_ratings[df_ratings.User_ID.isin(userSimilarity)] #rating for similar users\n",
    "            \n",
    "            recommendedMovies = bestMovies_similarUsers(users, movie_df, TP_movies)\n",
    "        \n",
    "\n",
    "        else: #if user didn't like the movie\n",
    "            bestRating = userRatings.Rating.max() #max rating\n",
    "    \n",
    "            if bestRating >= 3: #if the user liked movie with higest rating\n",
    "                bestRating_index = userRatings.Rating.argmax() #index of best rating\n",
    "                bestRating_movieID = userRatings.iloc[bestRating_index].Movie_ID #movieID of movie with best rating\n",
    "                bestMovie_title = movie_df[movie_df.Movie_ID == bestRating_movieID].Name.iloc[0] #title of best rated movie\n",
    "                \n",
    "                if bestMovie_title in seenMovies_titles:\n",
    "                    seenMovies_titles.remove(bestMovie_title) #remove bestMovie_title from list of seen movies, since we still need that\n",
    "            \n",
    "                if seenMovies_titles: #if any seen movies, remove movies already seen\n",
    "                    movies = TMDB_movies[~TMDB_movies.title.isin(seenMovies_titles)] #remove movies already seen\n",
    "                else:\n",
    "                    movies = TMDB_movies\n",
    "                \n",
    "                genreFiltering = filterByGenre(bestMovie_title, movies) #find movies with same genre as movieTitle\n",
    "                communityMovies = get_movie_community_df(bestMovie_title, genreFiltering) #find movies within same community\n",
    "                TP_movies = TopicModeling(communityMovies, bestMovie_title) #perform topic modeling, and return movies left after that\n",
    "\n",
    "                #find similar users \n",
    "                userSimilarity = similarUsers(userID, 20)\n",
    "                users = df_ratings[df_ratings.User_ID.isin(userSimilarity)] #rating for similar users\n",
    "\n",
    "                recommendedMovies = bestMovies_similarUsers(users, movie_df, TP_movies)\n",
    "                \n",
    "            else: #if the user didn't like any movies seen\n",
    "                #print(\"movieTitle\")\n",
    "                #find similar users \n",
    "                userSimilarity = similarUsers(userID, 20)\n",
    "                users = df_ratings[df_ratings.User_ID.isin(userSimilarity)] #rating for similar users\n",
    "            \n",
    "                recommendedMovies = bestMovies_similarUsers(users, movie_df, TMDB_movies)\n",
    "        \n",
    "    if not recommendedMovies:\n",
    "        \n",
    "        print(\"There are no recommendations for movies, that you haven't already seen\")\n",
    "    return recommendedMovies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d342d0",
   "metadata": {},
   "source": [
    "---\n",
    "# **Interactive System Design**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ecd43",
   "metadata": {},
   "source": [
    "The following code is a minimal interactive system that provides a GUI for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd920d2",
   "metadata": {},
   "source": [
    "**Defining global variables and constants in the class *MovieRecommendationApp*:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2494,
   "id": "49ade200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieRecommendationApp:\n",
    "    def __init__(self):\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Movie Recommendation\")\n",
    "        self.root.geometry(\"1200x800\")\n",
    "        self.text_light_color = \"snow\"\n",
    "        self.text_button_color = \"#cae8ff\"\n",
    "        self.text_dummy_color = \"gray73\"\n",
    "        self.text_font = \"Helvetica\"\n",
    "        self.text_size_very_big = 25\n",
    "        self.text_size_big = 20\n",
    "        self.text_size_medium = 15\n",
    "        self.text_size_small = 13\n",
    "        self.background_color = \"#181818\"\n",
    "        self.button_disabled_color = \"gray60\"\n",
    "        self.button_enabled_color = \"#004277\"\n",
    "        self.root.configure(bg=self.background_color)\n",
    "        self.colors_rating = [\"#f42b27\", \"#d3591d\", \"#eba600\", \"#58b327\", \"#44c200\"]\n",
    "        \n",
    "        self.user_id_entry = None\n",
    "        self.confirm_button = None\n",
    "        self.error_label = None\n",
    "        self.search_var = None\n",
    "        self.movie_listbox = None\n",
    "        self.search_entry = None\n",
    "        self.selected_rating = None\n",
    "        self.user_id = None\n",
    "        self.selected_movie = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e33bd1",
   "metadata": {},
   "source": [
    "**Back button**\n",
    "\n",
    "The back button is used on all of the screens except *`Screen 1`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2495,
   "id": "298f0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_button(self, command):\n",
    "    back_label = tk.Label(self.root, text=\"‹ Back\", font=(self.text_font, self.text_size_small), fg=self.button_disabled_color, bg=self.background_color, cursor=\"hand2\")\n",
    "    back_label.bind(\"<Button-1>\", lambda e: command())\n",
    "    back_label.place(relx=0.05, rely=0.05)\n",
    "\n",
    "MovieRecommendationApp.back_button = back_button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18493b13",
   "metadata": {},
   "source": [
    "### ***`Screen 1`*: User ID**\n",
    "This screen is the first screen the user sees when the application starts. The user can either\n",
    "1. Type in their *user ID* to show that they are already in our dataframe. This will redirect them to *`Screen 3`*\n",
    "2. Click on \"I do not have a user ID\". This will redirect them to *`Screen 2`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2496,
   "id": "37f249b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_user_id_screen(self):\n",
    "    # Clear screen\n",
    "    for widget in self.root.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "    # Ask for user ID\n",
    "    user_id_label = tk.Label(self.root, text=\"What is your user ID?\", bg=self.background_color, font=(self.text_font, self.text_size_big, \"bold\"), fg=self.text_light_color)\n",
    "    user_id_label.place(relx=0.5, rely=0.3, anchor=\"center\")\n",
    "\n",
    "    # User ID text field\n",
    "    self.user_id_var = tk.StringVar()\n",
    "    self.user_id_entry = tk.Entry(self.root, textvariable=self.user_id_var, font=(self.text_font, self.text_size_medium), fg=\"black\", cursor=\"xterm\")\n",
    "    self.user_id_entry.place(relx=0.5, rely=0.4, anchor=\"center\")\n",
    "\n",
    "    self.error_label = tk.Label(self.root, text=\"\", bg=self.background_color, font=(self.text_font, self.text_size_small), fg=\"red\")\n",
    "    self.error_label.place(relx=0.5, rely=0.45, anchor=\"center\")\n",
    "\n",
    "    # Frame for buttons\n",
    "    button_frame = tk.Frame(self.root, bg=self.background_color)\n",
    "    button_frame.place(relx=0.5, rely=0.6, anchor=\"center\")\n",
    "\n",
    "    # Confirm user ID button\n",
    "    self.confirm_button = tk.Button(button_frame, text=\"CONFIRM\", bg=self.button_disabled_color, command=self.confirm_user_id, font=(self.text_font, self.text_size_medium), fg=self.text_button_color, state=\"disabled\", relief=tk.FLAT)\n",
    "    self.confirm_button.grid(row=0, column=0, sticky=\"ew\", pady=(0, 10))\n",
    "    self.root.bind(\"<Return>\", lambda event: self.confirm_user_id() if self.confirm_button[\"state\"] == \"normal\" else None) # Bind the Enter key\n",
    "\n",
    "    # No user ID button\n",
    "    no_id_label = tk.Label(button_frame, text=\"I do not have a user ID\", font=(self.text_font, self.text_size_small, \"underline\"), fg=\"white\", bg=self.background_color, cursor=\"hand2\", bd=0)\n",
    "    no_id_label.bind(\"<Button-1>\", lambda e: self.show_recommendation_for_new_user())\n",
    "    no_id_label.grid(row=1, column=0, sticky=\"ew\")\n",
    "\n",
    "    button_frame.grid_columnconfigure(0, weight=1)\n",
    "\n",
    "    def check_user_id(*args):\n",
    "        if self.user_id_var.get():\n",
    "            self.confirm_button.config(state=\"normal\", bg=self.button_enabled_color, cursor=\"hand2\")\n",
    "        else:\n",
    "            self.confirm_button.config(state=\"disabled\", bg=self.button_disabled_color, cursor=\"arrow\")\n",
    "\n",
    "        if self.user_id_var.get().isdigit() or self.user_id_var.get() == \"\":\n",
    "            self.error_label.config(text=\"\")\n",
    "        else:\n",
    "            self.error_label.config(text=\"The ID is only a number.\", font=(self.text_font, 13))\n",
    "            self.user_id_var.set(self.user_id_var.get()[:-1])  # Remove the last character\n",
    "\n",
    "    self.user_id_var.trace(\"w\", check_user_id)\n",
    "\n",
    "MovieRecommendationApp.show_user_id_screen = show_user_id_screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2497,
   "id": "bd77dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirm_user_id(self):\n",
    "    user_id = self.user_id_entry.get()\n",
    "    if user_id.isdigit():  # Check if the user ID represents a valid integer\n",
    "        if int(user_id) not in np.genfromtxt('top_1000_user.csv', delimiter=','):\n",
    "            self.error_label.config(text=\"The user ID does not exist. Please enter a valid user ID.\")\n",
    "            self.error_label.place(relx=0.5, rely=0.45, anchor=\"center\")\n",
    "        else:\n",
    "            self.error_label.config(text=\"\")\n",
    "            self.user_id = user_id\n",
    "            self.show_movie_list_screen()\n",
    "    else: # Extra check\n",
    "        self.error_label.config(text=\"The user ID must be a number.\")\n",
    "        self.error_label.place(relx=0.5, rely=0.45, anchor=\"center\")\n",
    "\n",
    "MovieRecommendationApp.confirm_user_id = confirm_user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6c37d",
   "metadata": {},
   "source": [
    "### ***`Screen 2`*: Recommend movies for users with no user ID**\n",
    "In the absence of a user ID, it is assumed that no movies have been watched, as there is no data on the user’s history. Consequently, the screen displays the top five rated movies, and the filtering is therefore not applied to these movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2498,
   "id": "6adc9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_recommendation_for_new_user(self):\n",
    "    # Clear screen\n",
    "    for widget in self.root.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "    self.back_button(self.show_user_id_screen)\n",
    "\n",
    "    recommended_movies_header = tk.Label(self.root, text=\"As a new user we recommend the top-rated movies:\", bg=self.background_color, font=(self.text_font, self.text_size_big, \"bold\"), fg=self.text_light_color)\n",
    "    recommended_movies_header.place(relx=0.5, rely=0.3, anchor=\"center\")\n",
    "\n",
    "    # Recommendation for new user\n",
    "    recommendation_new_user = recommendation(None, None, None, df_ratings, df_movies, df_tmdb)\n",
    "    recommendation_text = \"\\n\".join(f\"{i+1}. {movie}\" for i, movie in enumerate(recommendation_new_user))\n",
    "    recommendation_label = tk.Label(self.root, text=recommendation_text, bg=self.background_color, font=(self.text_font, self.text_size_medium, \"italic\"), fg=self.text_light_color, justify=tk.LEFT)\n",
    "    recommendation_label.place(relx=0.5, rely=0.5, anchor=\"center\")\n",
    "    \n",
    "MovieRecommendationApp.show_recommendation_for_new_user = show_recommendation_for_new_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735dd85",
   "metadata": {},
   "source": [
    "### ***`Screen 3.A`*: Choosing a movie**\n",
    "This screen shows a list of movies that the user can choose as a movie they have watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2499,
   "id": "c7a63dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for the searching of the movies in the list\n",
    "\n",
    "def update_movie_list(self):\n",
    "    # Check if the listbox exists before trying to update it\n",
    "    if self.movie_listbox is not None:\n",
    "        search_query = self.search_var.get().lower()\n",
    "        self.movie_listbox.delete(0, tk.END)\n",
    "        for movie in sorted(df_tmdb[\"title\"].values):\n",
    "            if search_query in movie.lower():\n",
    "                self.movie_listbox.insert(tk.END, movie)\n",
    "\n",
    "def clear_search(self, event):\n",
    "    # Clear the search field when it receives focus\n",
    "    if self.search_entry.get() == \"Search\":\n",
    "        self.search_entry.delete(0, tk.END)\n",
    "\n",
    "def restore_search(self, event):\n",
    "    # Restore the placeholder text when the field loses focus\n",
    "    if not self.search_entry.get():\n",
    "        self.search_entry.insert(0, \"Search\")\n",
    "\n",
    "MovieRecommendationApp.update_movie_list = update_movie_list\n",
    "MovieRecommendationApp.clear_search = clear_search\n",
    "MovieRecommendationApp.restore_search = restore_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2500,
   "id": "84fc9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_movie_list_screen(self):\n",
    "\n",
    "    # Clear screen\n",
    "    for widget in self.root.winfo_children():\n",
    "        widget.destroy()\n",
    "    \n",
    "    self.back_button(self.show_user_id_screen)\n",
    "    \n",
    "    # Question\n",
    "    watched_movie_label = tk.Label(self.root, text=\"Which movie have you watched?\", bg=self.background_color, font=(self.text_font, self.text_size_big, \"bold\"), fg=self.text_light_color)\n",
    "    watched_movie_label.place(relx=0.5, rely=0.2, anchor=\"center\")\n",
    "    \n",
    "    # Show the search label and field\n",
    "    search_var = StringVar()\n",
    "    search_var.trace(\"w\", lambda *args: self.update_movie_list())\n",
    "    search_label = tk.Label(self.root, text=\"Search\", bg=self.background_color, font=(self.text_font, self.text_size_medium), fg=self.text_dummy_color)\n",
    "    search_label.place(relx=0.41, rely=0.32, anchor=\"e\")\n",
    "    search_entry = tk.Entry(self.root, textvariable=search_var, font=(self.text_font, self.text_size_medium), fg=self.text_dummy_color)\n",
    "    search_entry.place(relx=0.435, rely=0.32, anchor=\"w\")\n",
    "    \n",
    "    # Show the scrollable list of movies\n",
    "    frame = tk.Frame(self.root)\n",
    "    frame.place(relx=0.5, rely=0.5, anchor=\"center\")\n",
    "    scrollbar = Scrollbar(frame)\n",
    "    scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "    movie_listbox = Listbox(frame, yscrollcommand=scrollbar.set, width=50, cursor=\"hand2\")\n",
    "    for movie in sorted(df_tmdb[\"title\"].values):\n",
    "        movie_listbox.insert(tk.END, movie)\n",
    "    movie_listbox.pack(side=tk.LEFT, fill=tk.BOTH)\n",
    "    scrollbar.config(command=movie_listbox.yview)\n",
    "    \n",
    "    # Assign the widgets to the instance variables\n",
    "    self.movie_listbox = movie_listbox\n",
    "    self.search_var = search_var\n",
    "    self.search_entry = search_entry\n",
    "    \n",
    "    def on_continue():\n",
    "            self.selected_movie = movie_listbox.get(movie_listbox.curselection())  # Save the selected movie\n",
    "            self.show_rating_screen()  # Go to the next sscreen\n",
    "\n",
    "    continue_button = tk.Button(self.root, text=\"CONTINUE\", bg=self.button_disabled_color, command=on_continue, font=(self.text_font, self.text_size_medium), fg=self.text_button_color, state=\"disabled\")  # Disable the button initially\n",
    "    continue_button.place(relx=0.5, rely=0.7, anchor=\"center\")\n",
    "    self.root.bind(\"<Return>\", lambda event: on_continue() if continue_button[\"state\"] == \"normal\" else None) # Bind the Enter key\n",
    "    \n",
    "    def update_button_state(event):\n",
    "        if movie_listbox.curselection():\n",
    "            continue_button.config(state=\"normal\", bg=self.button_enabled_color, cursor=\"hand2\")\n",
    "        else:\n",
    "            continue_button.config(state=\"disabled\", bg=self.button_disabled_color, cursor=\"arrow\")\n",
    "\n",
    "    movie_listbox.bind(\"<<ListboxSelect>>\", update_button_state)\n",
    "\n",
    "    if \"error_label\" in globals() and self.error_label.winfo_exists():\n",
    "        self.error_label.place_forget()\n",
    "        \n",
    "MovieRecommendationApp.show_movie_list_screen = show_movie_list_screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5568af9a",
   "metadata": {},
   "source": [
    "### ***`Screen 3.B`*: Rate selected movie**\n",
    "This screen presents a numerical rating system ranging from 1 to 5, allowing users to select a value within this range to rate the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2501,
   "id": "9479e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rating_screen(self):\n",
    "    # Clear screen\n",
    "    for widget in self.root.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "    self.back_button(self.show_movie_list_screen)\n",
    "\n",
    "    # Question\n",
    "    rating_question_label = tk.Label(self.root, text=\"How would you rate the movie on a scale of 1 to 5?\", bg=self.background_color, font=(self.text_font, self.text_size_big, \"bold\"), fg=self.text_light_color)\n",
    "    rating_question_label.place(relx=0.5, rely=0.2, anchor=\"center\")\n",
    "\n",
    "    # Rating buttons\n",
    "    rating_buttons = []\n",
    "    for i in range(1, 6):\n",
    "        rating_button = tk.Button(self.root, text=str(i), bg=self.background_color, activebackground=self.colors_rating[i-1], bd=1, font=(self.text_font, self.text_size_medium), fg=self.colors_rating[i-1], width=4, cursor=\"hand2\", relief=tk.FLAT)\n",
    "        rating_button.place(relx=0.2 + 0.1*i, rely=0.5, anchor=\"center\")\n",
    "        rating_buttons.append(rating_button)\n",
    "\n",
    "    # Highlight clicked rating button and save the rating\n",
    "    def select_rating(i):\n",
    "        self.selected_rating = i + 1  # Save selected rating\n",
    "        for j, button in enumerate(rating_buttons):\n",
    "            if j == i:\n",
    "                button.config(bg=self.colors_rating[j], fg=\"white\")  # Fill the selected button\n",
    "            else:\n",
    "                button.config(bg=self.background_color, fg=self.colors_rating[j])  # Remove fill from other buttons\n",
    "        self.confirm_button.config(state=\"normal\", bg=self.button_enabled_color, cursor=\"hand2\")  # Enable the \"Confirm\" button\n",
    "\n",
    "    for i, button in enumerate(rating_buttons):\n",
    "        button.config(command=lambda i=i: select_rating(i))\n",
    "\n",
    "    # Show the rating indicators\n",
    "    poor_label = tk.Label(self.root, text=\"Poor\", bg=self.background_color, font=(self.text_font, self.text_size_medium), fg=self.text_light_color)\n",
    "    poor_label.place(relx=0.22, rely=0.5, anchor=\"center\")\n",
    "    excellent_label = tk.Label(self.root, text=\"Excellent\", bg=self.background_color, font=(self.text_font, self.text_size_medium), fg=self.text_light_color)\n",
    "    excellent_label.place(relx=0.8, rely=0.5, anchor=\"center\")\n",
    "\n",
    "    # Confirm button\n",
    "    def on_confirm():\n",
    "        self.confirm_button.config(state=\"disabled\", bg=self.button_disabled_color, cursor=\"watch\")\n",
    "\n",
    "        please_wait_label = tk.Label(self.root, text=\"Please wait...\", bg=self.background_color, font=(self.text_font, self.text_size_medium), fg=self.text_light_color)\n",
    "        please_wait_label.place(relx=0.5, rely=0.8, anchor=\"center\")\n",
    "\n",
    "        self.root.config(cursor=\"arrow\") # Change cursor to hourglass\n",
    "        self.root.update_idletasks() # Update the GUI\n",
    "\n",
    "        # Call the recommendation function and show the recommendation screen\n",
    "        recommendations = recommendation(int(self.user_id), self.selected_rating, self.selected_movie, df_ratings, df_movies, df_tmdb)\n",
    "        self.show_recommendation_screen(recommendations)\n",
    "        \n",
    "        self.root.config(cursor=\"\") # Change curser back\n",
    "\n",
    "\n",
    "    self.confirm_button = tk.Button(self.root, text=\"CONFIRM\", bg=self.button_disabled_color, command=on_confirm, font=(self.text_font, self.text_size_medium), fg=self.text_button_color, state=\"disabled\", relief=tk.FLAT)\n",
    "    self.confirm_button.place(relx=0.5, rely=0.7, anchor=\"center\")\n",
    "    self.root.bind(\"<Return>\", lambda event: on_confirm() if self.confirm_button[\"state\"] == \"normal\" else None) # Bind the Enter key\n",
    "\n",
    "MovieRecommendationApp.show_rating_screen = show_rating_screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33207750",
   "metadata": {},
   "source": [
    "### ***`Screen 3.C`*: Recommend movies for existing user**\n",
    "This screen shows a selection of five films, recommended based on the user’s chosen film and the corresponding rating they have assigned to it. The movies are sorted based on the top similar users; i.e. other users who has the shortest cosine distance to the user that is using the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2502,
   "id": "a62fcd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_recommendation_screen(self, recommendations):\n",
    "    # Clear screen\n",
    "    for widget in self.root.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "    self.back_button(self.show_rating_screen)\n",
    "\n",
    "    recommended_movies_header_1 = tk.Label(self.root, text=\"You have watched the movie\", bg=self.background_color, font=(self.text_font, self.text_size_big, \"bold\"), fg=self.text_light_color)\n",
    "    recommended_movies_header_1.place(relx=0.5, rely=0.2, anchor=\"center\")\n",
    "    \n",
    "    recommended_movies_header_2 = tk.Label(self.root, text=f\"{self.selected_movie}\", bg=self.text_light_color, font=(self.text_font, self.text_size_very_big, \"bold\"), fg=\"black\")\n",
    "    recommended_movies_header_2.place(relx=0.5, rely=0.25, anchor=\"center\")\n",
    "    \n",
    "    selected_rating_color = self.colors_rating[self.selected_rating - 1]\n",
    "    recommended_movies_header_3_text = tk.Label(self.root, text=\"and rated it \", bg=self.background_color, font=(self.text_font, self.text_size_big, \"bold\"), fg=self.text_light_color)\n",
    "    recommended_movies_header_3_text.place(relx=0.56, rely=0.31, anchor=\"e\")\n",
    "    recommended_movies_header_3_rating = tk.Label(self.root, text=f\"{self.selected_rating}\", bg=self.background_color, font=(self.text_font, self.text_size_big, \"bold\"), fg=selected_rating_color)\n",
    "    recommended_movies_header_3_rating.place(relx=0.56, rely=0.31, anchor=\"w\")\n",
    "\n",
    "    recommended_movies_header_4 = tk.Label(self.root, text=\"Therefore, we recommend that you watch:\", bg=self.background_color, font=(self.text_font, self.text_size_big, \"bold\"), fg=self.text_light_color)\n",
    "    recommended_movies_header_4.place(relx=0.5, rely=0.4, anchor=\"center\")\n",
    "    \n",
    "    # Recommended movie\n",
    "    recommendation_text = \"\\n\".join(f\"{i+1}. {movie}\" for i, movie in enumerate(recommendations))\n",
    "    recommendation_label = tk.Label(self.root, text=recommendation_text, bg=self.background_color, font=(self.text_font, self.text_size_medium, \"italic\"), fg=self.text_light_color, justify=tk.LEFT)\n",
    "    recommendation_label.place(relx=0.5, rely=0.5, anchor=\"center\")\n",
    "    \n",
    "    # \"Try again\" button -> Redirect to show_movie_list_screen\n",
    "    try_again_button = tk.Button(self.root, text=\"Try again\", bg=self.button_enabled_color, command=self.show_movie_list_screen, font=(self.text_font, self.text_size_medium), fg=self.text_button_color, cursor=\"hand2\")\n",
    "    try_again_button.place(relx=0.5, rely=0.8, anchor=\"center\")\n",
    "\n",
    "MovieRecommendationApp.show_recommendation_screen = show_recommendation_screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e2730",
   "metadata": {},
   "source": [
    "### **Run the application**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2503,
   "id": "4fa493a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = MovieRecommendationApp()\n",
    "app.show_user_id_screen()\n",
    "app.root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
